\contentsline {figure}{\numberline {1}{\ignorespaces Domain decomposition using $p=8$ processes. In the scenario (a), a straightforward 3D decomposition divides every dimension in $p^{1/3}=2$. In the scenario (b), COMM starts by finding an optimal sequential schedule and then parallelizes it minimizing crossing dependencies. The total communication volume is reduced by 17\% compared to the former strategy.\relax }}{15}{figure.caption.5}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {3D domain decomposition}}}{15}{subfigure.1.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {COMM decomposition}}}{15}{subfigure.1.2}
\contentsline {figure}{\numberline {2}{\ignorespaces Achieving data movement optimal MMM in eleven simple steps: (I) I/O lower bounds and red-blue pebble game, (II) $2S$-partition lemma, (III) reuse-based lemma, (IV) MMM I/O lower bound, (V) I/O optimal schedule, (VI) communication optimal parallel schedule, (VII) parallel efficiency analysis, (VIII) buffer optimization, (IX) communication optimization, (X) process decomposition optimization, (XI) best time-to-solution result. \relax }}{16}{figure.caption.6}
\contentsline {figure}{\numberline {3}{\ignorespaces Different parallelization schemes of matrix multiplication arriving from the optimal sequential schedule. Up to six processes may be used optimally - above this limit we either increase I/O or communication. Shades of gray represent local domains of different processes. a) Global iteration domain (pink) and the optimal subset shape (green). b) Global iteration space scheduling (arrows represent data dependencies). c) Optimal parallelization using $p=3$ processes. d-f) Suboptimal parallelization using $p=24$ processes. From left to right, \textbf {par in ij}, \textbf {par in ijk} and \textbf {cubic}. Note the trade-off between I/O (shrinkage of local domain) and communication (dashed red lines showing parallelization and required communication in \textbf {k} dimension).\relax }}{17}{figure.caption.7}
\contentsline {figure}{\numberline {4}{\ignorespaces Parallel efficiency of different parallelization schemes. Three vertical dashed lines correspond to thresholds $p1 = mn/S$, $p2 = mnk/S^{3/2}$ and $p3 = (\frac {3}{S})^{3/2}mnk$ (Table \ref {tab:mmmEfficiency}). Note that for cases (b) and (c), increasing memory $S$ six times reduces the number of processes required to fully saturate it drops from $p2=741455$ to $p2=50449$. \relax }}{18}{figure.caption.8}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {"flat" matrices}}}{18}{subfigure.4.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {square matrices, small $S$}}}{18}{subfigure.4.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {square matrices, large $S$}}}{18}{subfigure.4.3}
\contentsline {figure}{\numberline {5}{\ignorespaces Process decomposition for square matrices and 65 processes. To utilize all resources, the local domain is drastically stretched (a). Dropping one process results in a symmetric grid (b) that increases the computation per process only by 1\%, but reduces the communication by 36\%.\relax }}{19}{figure.caption.9}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {$1 \times 3 \times 15$ grid}}}{19}{subfigure.5.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {$4 \times 4 \times 4$ grid with one idle process}}}{19}{subfigure.5.2}
\contentsline {figure}{\numberline {6}{\ignorespaces An MMM CDAG forming a 3D iteration space $\mathcal {V} \subset \mathbb {Z}^3$. An input matrix A (blue vertices) is represented as its projection $\alpha = \phi _{ik}(\mathcal {V})$ on an \textbf {ik} plane - similarly, an input matrix B (red vertices) is a projection $\beta = \phi _{kj}(\mathcal {V})$ on the \textbf {kj} plane and an output matrix C (light yellow) is a projection $\gamma = \phi _{ij}(\mathcal {V})$ on the \textbf {ij} plane. Each vertex in this iteration space (except of the vertices in the bottom layer) has three parents - blue, red, and yellow and one yellow child (except of vertices in the top layer). $\alpha \cup \beta \cup \gamma $ form the dominator set $Dom(V_i)$. subcomputation $V_i \subset \mathcal {V}$ of an $S$-partition must satisfy $|Dom(V_i)| = |\alpha | + |\beta | + |\gamma | \le S$ (number of inputs must be smaller than $S$. \ref {fig:f22} optimal surface to volume subset shape. Note that in a subsequent subset computation only one of the three planes (blue, red or yellow) can be reused. \ref {fig:f23} the optimal subset shapes when data reuse is considered. Observe that even though $|V_i| > |V_j|$, but $|V_i|/(|\alpha _i| + |\beta _i|) < |V_j|/(|\alpha _j| + |\beta _j|)$.\relax }}{19}{figure.caption.10}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {MMM CDAG}}}{19}{subfigure.6.1}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {cubic decomposition}}}{19}{subfigure.6.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {"flat" decomposition}}}{19}{subfigure.6.3}
